import os
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
from datetime import timedelta, datetime
from dateutil.relativedelta import relativedelta
import logging as log
import numpy as np
from airflow.providers.amazon.aws.hooks.s3 import S3Hook


""" ETL dag 
--Extract data from Facultad Latinoamericana De Ciencias Sociales Postgres database
--Transform the data with pandas
--Load the data to a AWS s3 database"""

log.basicConfig(level=log.ERROR,
                format='%(asctime)s - %(processName)s - %(message)s',
                datefmt='%Y-%m-%d')
#set file name
file_name = "{{file_name_config}}"

#Postgre query
def query():

    DIR = os.path.dirname(os.path.normpath(__file__)).rstrip('/dags')
    sql_file = "{{sql_file_route}}"
    FILE = f"{DIR}" + f"{sql_file}"
    
    try:
        pg_hook = PostgresHook(postgres_conn_id='db_universidades_postgres', schema="training")
    except Exception as e:
        log.error(e)
        raise e


    
    
    #set path
    file_name_main = os.path.join(os.path.dirname(__file__), '../files/'+file_name+'.csv')

    #open sql file

    j = open(FILE)

    j = j.read()
    
    #transform query
    sql_query = "COPY ( \n{0}\n ) TO STDOUT WITH CSV HEADER".format(j.replace(";", "")) 
    
    #get and save the file
    pg_hook.copy_expert(sql_query, file_name_main)

def pandas_process_func():
    DIR = os.path.dirname(os.path.normpath(__file__)).rstrip('/dags')

    try:
        df = pd.read_csv(f"{DIR}/files/"+file_name+".csv", encoding='UTF-8')
        cp = pd.read_csv(f"{DIR}/assets/codigos_postales.csv", encoding='UTF-8' )
    except Exception as e:
        log.error(e)
        raise e

    cp.rename(columns= {'codigo_postal': 'postal_code', 'localidad': 'location'}, inplace = True)

    df["university"] = df["university"].str.lower()
    df["career"] = df["career"].str.lower()
    df["last_name"] = df["last_name"].str.lower()
    df["gender"] = df["gender"].str.lower()
    df["email"] = df["email"].str.lower()
    df["inscription_date"] = df["inscription_date"].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))

    #strip spaces
    df["university"] = df["university"].str.strip()
    df["career"] = df["career"].str.strip()
    df["last_name"] = df["last_name"].str.strip()
    df["email"] = df["email"].str.strip()

    #select correct df
    if (df["postal_code"].isnull().all()) == True:
        #to string and lower
        df["location"] = df["location"].str.lower()
        cp["location"] = cp["location"].str.lower()
        cp["postal_code"] = cp["postal_code"]

        #strip spaces
        df["location"] = df["location"].str.strip()
        cp["location"] = cp["location"].str.strip()

        columns = ['university', 'career','last_name', 'location', 'postal_code']
        for i in columns:
            df[f"{i}"] = df[f"{i}"].replace(['-',],' ', regex=True)

        #merge
        mrg = pd.merge(
            df['location'],
            cp.drop_duplicates(subset=['location']),
            how="left",
            left_on='location',
            right_on='location'
        )
        df['postal_code'] = mrg['postal_code']#.replace(np.NaN,'')
        df['location'] = mrg['location']#.replace(np.NaN,'')
        df['postal_code'] = df['postal_code'].astype(str)

    else:
        #to string and lower
        df["postal_code"] = df["postal_code"].astype(str)
        cp["location"] = cp["location"].str.lower()
        cp["postal_code"] = cp["postal_code"].astype(str)

        #strip spaces
        df["postal_code"] = df["postal_code"].str.strip()
        cp["location"] = cp["location"].str.strip()

        columns = ['university', 'career','last_name', 'location', 'postal_code']
        for i in columns:
            df[f"{i}"] = df[f"{i}"].replace(['-',],' ', regex=True)

        #merge
        mrg = pd.merge(
            df['postal_code'],
            cp.drop_duplicates(subset=['postal_code']),
            how="left",
            left_on='postal_code',
            right_on='postal_code'
        )
        df['location'] = mrg['location']#.replace(np.NaN,'')
        df['postal_code'] = mrg['postal_code']#.replace(np.NaN,'')
        df['postal_code'] = df['postal_code'].astype(str)

    

    #setting gender 
    df['gender'] = df['gender'].replace('m', 'male')
    df['gender'] = df['gender'].replace('f', 'female')

    #age
    if len(df['birthdate'].loc[0]) == 10:
        df["birthdate"] =  pd.to_datetime(df["birthdate"], dayfirst=True)
        now = pd.Timestamp('now')
        df['age'] = (now - df['birthdate']).astype('<m8[Y]')
        df["age"] = df["age"].astype(int)
    else:
        df.drop(df.loc[df['birthdate']=='29-Feb-10'].index, inplace=True)
        df["birthdate"] = pd.to_datetime(df["birthdate"], yearfirst=True)
        max_year = datetime.now().year - 12
        df["birthdate"] = df["birthdate"].apply(lambda x : x - relativedelta(years=100) if x.year > max_year else x)
        df['age'] = df["birthdate"].apply(lambda x: (relativedelta(datetime.now(), x).years))

    #creating the file
    try:
        df.to_csv(f"{DIR}/datasets/"+file_name+".csv")
    except Exception as e:
        log.error(e)
        raise e
    

def upload_to_s3():
    """This function upload the csv file to AWS S3 DB with a S3Hook"""

    log.info("Uploading cvs files")
    DIR = os.path.dirname(os.path.normpath(__file__)).rstrip('/dags')
    #S3 Hook
    try:
        s3_hook = S3Hook("universidades_S3")
    except Exception as e:
        log.error(e)
        raise e
    #Load to S3
    try:
        s3_hook.load_file(
            filename= f"{DIR}/datasets/"+file_name+".csv",
            key= file_name+".csv",
            bucket_name="cohorte-agosto-38d749a7",
            replace=True
        )
    except Exception as e:
        log.error(e)
        raise e

#retry 5 times with a delay of 5 seconds
default_args = {
    'retries': 5,
    'retry_delay': timedelta(seconds=5),
}

with DAG(
    "{{dag_id}}",
    default_args = default_args,
    description = '{{description}}',
    schedule_interval = timedelta(hours = 1),
    start_date = datetime(2022, 8, 22)
) as dag:
    query_sql = PythonOperator(task_id = 'query_sql',
                               python_callable = query) 
    pandas_process = PythonOperator(task_id = 'pandas_process',
                                    python_callable = pandas_process_func)   
    load_S3 = PythonOperator(task_id = 'load_S3',
                                python_callable = upload_to_s3) 


    query_sql >> pandas_process >> load_S3
